In this paper we have seen that \citeauthor{li2018deep} propose a new neural network architecture that involves the learning of prototypes to allow for easier interpretation of predictions in response to calls to more transparency in deep learning models. We have succeeded in reproducing the results attained by Li et al. \citep{li2018deep}\footnote{The code that was used for reproducing the original paper can be found at: \url{https://github.com/Sasafrass/FACT-AI}} and therefore we award the results reproduced badge\footnote{\url{https://www.acm.org/publications/policies/artifact-review-badging}} to the paper. Furthermore, we have extended their architecture by replacing the prototypes with hierarchical prototypes. These hierarchical prototypes add another layer of explainability to the model, with superprototypes showing the "class average" while subprototypes capture intraclass differences. In doing so, we can visualize several layers consisting of more intricate features allowing for better interpretation of why our network is making certain predictions. We have shown that we can enforce our model to learn representations for all superclasses while simultaneously learning the subclasses without sacrificing our model's predictive abilities. Ultimately, we have also demonstrated ways in which our model's increased interpretability may help in other domains such as fairness, where our model can be used to uncover potentially biased predictions in order to counteract these unwanted biases. 

% Oh hi
% Whaddup
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
