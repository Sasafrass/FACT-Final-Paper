Our primary goal is to provide an architecture that is both accurate and interpretable, which places our architecture within the domains of Transparency and Accountability. Another domain that has been getting more attention recently, however, is that of Fairness. Algorithms can be considered fair if they do not give disparate treatment to a certain disadvantaged group. Fairness criteria that are supposed to effect this are, according to \citet{fairnessgeneral}, "clearly intended to protect the disadvantaged group", even though they are not always succesful in improving the well-being of the this group \citep{fairnessgeneral}.

An important part of fairness is determining which algorithms are actually fair and which are biased. Bringing these biases to light in existing models has led, for example, to reduced disparity in hiring processes \citep{accountabilitycompanies}. Exposing biases is something that interpretable models aid in. In the case of the architecture of \citet{li2018deep} and our own architecture, for example, prototypes provide archetypical examples of data classes. When applied to, say, hiring data, prototypes show if the archetypical person that gets hired tends to be of a certain gender, race, or other sensitive category. If a model is found to be biased, appropriate action can be taken.

On the other hand, our architecture is not capable of downplaying or removing certain information from the input. This is necessary if the architecture is to be used for fair classification that does not rely on sensitive information present in the input. Simply removing features explicitly related to this sensitive information is not enough, since the information can be inferred from other correlated features \citep{unawarenesssucks}.

Both interclass and intraclass differences may be used to check for biases in the model or the data it is trained on. A certain class may be underrepresented when it has few subprototypes, or may have a much higher variance in its subprototypes. Although these variations are not related to fairness when it comes to MNIST digits, they may become relevant when a prototype network is trained on data containing sensitive information. P

A variant of the variational autoencoder architecture \citep{vae} proposed by Louizos et al. \citep{vfae} is capable of transforming input to a latent space in such a way that information can be factored out of the latent representation, while maintaining as much information as possible that is useful for classification. One potential way of making the architecture we propose in this paper fairer is by replacing the autoencoder with a Variational Fair Autoencoder. The prototypes would also need to be changed from points to distributions in the latent space. We believe that this will result in prototypes that are unbiased toward the sensitive information.

