\begin{itemize}
    \item Introduce the field of transparency.
    \item Describe problem that the prototype algorithm solves.
\end{itemize}

The importance of transparency in Machine Learning is rapidly increasing. As Machine Learning algorithms grow in prominence in society, being able to explain the output of these algorithms becomes more and more of a requirement. This is true both from a legal angle (such as with the GDPR \citep{gdprart}) as from a human angle -- interpretability engenders trust in users, even when it makes mistakes \citep{automtrust}, which is especially important when people rely on the algorithm to keep them healthy and safe, as is the case with medical AI or self-driving cars. Furthermore, an explainable algorithm allows us to better find biases and spurious relationships present in the training data, e.g. because of data leakage \citep{dataleakage}. However, due to the highly complex nature of artificial neural networks that most modern Machine Learning algorithms rely on, giving satisfactory explanations of model output is similarly complex.

One method of explaining why a model classifies inputs the way it does is by visualizing what a model considers to be an archetypal example of a class. Activation Maximization (AM) \citep{activationmaximization} is such a method: it provides an input that would produce the highest activation for a certain class. However, from a human point of view, such inputs tend to be dissimilar to actual inputs from that class. This goes against the intuition that model explanations should be interpretable as well as accurate. To combat this dissimilarity, Li et al. \citep{li2018deep} propose an architecture based around classification through measuring similarity to prototypes. Prototypes are latent representations of a class that are forced, through regularization, to be similar to inputs corresponding to that class. 

A number of other ways of addressing the lack of transparency also exist. One example of those is the explanation technique LIME \citep{LIME}, which provides local (i.e. for a single input) explanations of what features are important for the classification. When applied to images, for example, it highlights which pixels weighed heavily in its decision-making. This technique is binary -- a feature either was important or it was not. Other examples of methods that focus on explanation through feature highlighting have been recently proposed by Lundburg \& Lee \citep{featureexamplepaper1}. One drawback of the methods of improving explanation discussed so far is that they require new models to be trained in order to make explanation possible. A method that does not have this issue is Integrated Gradients, introduced by Sundararajan et al. \citep{axioms}.

In this paper, we first of all attempt to reproduce the results attained by the architecture introduced by Li et al. \citep{li2018deep}. Secondly, we extend the architecture with hierarchical prototypes. This hierarchy is composed of two layers: superprototypes and subprototypes. Subprototypes are close to actual encoded input (similar to prototypes in the original architecture), while superprototypes act as descriptors for the subprototype clusters in the latent space. For example, when training on the MNIST dataset, subprototypes for the numbers 4 and 9 may be close together, but far apart from other numbers. In such a case, a superprototype would come to cover both 4s and 9s. By visualizing the sub- and superprototypes an encoded input is closest to, we gain an extra layer of interpretability for the classification process. Furthermore, since the superprototypes represent a set of subprototypes that are close together, they can help explain which classes are similar to each other. This, in turn, can aid in understanding which classes often get mixed up and why, and allows us to take steps to avoid these misclassifications. (EXPLAIN MORE ABOUT THAT HERE). 

