The importance of transparency in Machine Learning is rapidly increasing. As Machine Learning algorithms grow in prominence in society, being able to explain the decision making process of these algorithms in an intelligible manner has become more important. This is true both from a legal perspective (such as with the GDPR \citep{gdprart}) as from a human perspective -- interpretability engenders trust in users, even when it makes mistakes \citep{automtrust}, which is especially important when people rely on the algorithm to keep them healthy and safe, as is the case with medical AI or self-driving cars. Furthermore, an explainable algorithm allows us to better detect biases and spurious relationships present in the training data, e.g. because of data leakage \citep{dataleakage}. However, most modern Machine Learning algorithms rely on artificial neural networks which use nonlinearities to capture hidden relations between inputs and outputs. This makes explaining model output in a satisfactory way complex \cite{NNblackboxexplanation}.

A number of different approaches have been proposed to address the inherent lack of transparency prevalent in widely used neural network architectures. One example of those approaches is the explanation technique LIME \citep{LIME}, which provides local (i.e. for a single input) explanations of what features are important for the classification. When applied to images, for example, it highlights which pixels weighed heavily in its decision-making. This technique is binary -- a feature was either a relevant factor or not. Other examples of methods that focus on explanation through feature highlighting have been recently proposed by Lundburg \& Lee \citep{featureexamplepaper1}. One drawback of the methods of improving explanation discussed so far is that they require new models to be trained in order to make explanation possible. A method that does not have this issue is Integrated Gradients \citep{axioms}, which also does feature highlighting but can be applied to existing models.

A more specific method of explaining why a model classifies inputs in a certain way is by visualizing what a model considers to be an archetypal example of a given class. Activation Maximization (AM) \citep{activationmaximization} is such a method: for each class it provides a hypothetical input that would produce the highest activation for that class. However, from a human point of view, such inputs tend to be dissimilar to actual inputs from that class  \citep{activationmaximization}. This goes against our intuition that model explanations should be interpretable as well as accurate. To resolve this dissimilarity, Li et al. \citep{li2018deep} propose a representation learning architecture for a \textit{prototype classifier}. This architecture classifies by measuring distance between input and prototypes, which are defined as points that represent a class in a latent space. These are learned through regularization such that they are close to encoded inputs corresponding to that class. Li et al. simultaneously train this classifier and an autoencoder \citep{autoencoderpaper} that transforms inputs to the latent space of the prototypes. After training has finished, the autoencoder can be used to visualize the prototypes by decoding them.

In this paper, we first of all attempt to reproduce the results attained by the architecture introduced by Li et al. \citep{li2018deep}. Secondly, we extend the architecture with hierarchical prototypes. This hierarchy is composed of two layers: superprototypes and subprototypes. Subprototypes are close to actual encoded input (similar to prototypes in the original architecture), while superprototypes act as descriptors for the subprototype clusters in the latent space. For example, when training on the MNIST dataset, subprototypes can find different ways of writing the same digit (as prototypes do in \citep{li2018deep}), while superprototypes come to represent the average way numbers are written. By visualizing the sub- and superprototypes an encoded input is closest to, we gain an extra layer of interpretability for the classification process. When the number of superprototypes is equal to the number of classes, a fixed activation pattern based on closest superprototype can be used to force the detection of all classes. The subprototypes will in this case take care of intraclass variation. The network automatically learns which classes need more subprototypes: in MNIST terminology, there may be more distinct ways to write a 2 than there are ways to write a 9. This solves an issue that the original architecture \citep{li2018deep} had, namely that when the number of prototypes exceeded the number of classes, there were cases where not all classes were represented by a prototype (for an example, see Figure~\ref{fig:non3} in the Appendix).

In the next section, we will briefly explain the original, nonhierarchical architecture. This is followed by a detailed description of our hierarchical prototype network. In Section~\ref{sec:setup}, we describe and justify our choice of hyperparameters. Results for both reproduction and the new architecture are examined in Section~\ref{sec:results}, followed by a discussion of both the benefits and shortcomings of (hierarchical) prototype networks. Finally, Section~\ref{sec:broad} discusses some broader implications within the field of transparancy and fairness in artificial intelligence. 
