The importance of transparency in Machine Learning is rapidly increasing. As Machine Learning algorithms grow in prominence in society, being able to explain the output of these algorithms becomes more and more of a requirement. This is true both from a legal angle (such as with the GDPR \citep{gdprart}) as from a human angle -- interpretability engenders trust in users, even when it makes mistakes \citep{automtrust}, which is especially important when people rely on the algorithm to keep them healthy and safe, as is the case with medical AI or self-driving cars. Furthermore, an explainable algorithm allows us to better find biases and spurious relationships present in the training data, e.g. because of data leakage \citep{dataleakage}. However, most modern Machine Learning algorithms rely on artificial neural networks which use nonlinearities to capture hidden relations between inputs and outputs. This makes explaining model output in a satisfactory way complex \citep{NNblackboxexplanation}.

One method of explaining why a model classifies inputs in a certain way is by visualizing what a model considers to be an archetypal example of a class. Activation Maximization (AM) \citep{activationmaximization} is such a method: for each class it provides a hypothetical input that would produce the highest activation for that class. However, from a human point of view, such inputs tend to be dissimilar to actual inputs from that class  \citep{activationmaximization}. This goes against our intuition that model explanations should be interpretable as well as accurate. To resolve this dissimilarity, Li et al. \citep{li2018deep} propose a representation learning architecture for a \textit{prototype classifier}. This architecture classifies by measuring distance to prototypes, which are defined as points that represent a class in a latent space. These are learned through regularization such that they are close to encoded inputs corresponding to that class. Li et al. simultaneously train this classifier and an autoencoder \citep{autoencoderpaper} that transforms inputs to the latent space of the prototypes. When this training is done, the autoencoder can be used to visualize the prototypes by decoding them.

A number of other ways of addressing the lack of transparency also exist. One example of those is the explanation technique LIME \citep{LIME}, which provides local (i.e. for a single input) explanations of what features are important for the classification. When applied to images, for example, it highlights which pixels weighed heavily in its decision-making. This technique is binary -- a feature either was important or it was not. Other examples of methods that focus on explanation through feature highlighting have been recently proposed by Lundburg \& Lee \citep{featureexamplepaper1}. One drawback of the methods of improving explanation discussed so far is that they require new models to be trained in order to make explanation possible. A method that does not have this issue is Integrated Gradients \citep{axioms}, which also does feature highlighting but can be applied to existing models.

In this paper, we first of all attempt to reproduce the results attained by the architecture introduced by Li et al. \citep{li2018deep}. Secondly, we extend the architecture with hierarchical prototypes. This hierarchy is composed of two layers: superprototypes and subprototypes. Subprototypes are close to actual encoded input (similar to prototypes in the original architecture), while superprototypes act as descriptors for the subprototype clusters in the latent space. For example, when training on the MNIST dataset, subprototypes can find different ways of writing the same digit (as prototypes do in \citep{li2018deep}), while superprototypes come to represent the average way numbers are written. By visualizing the sub- and superprototypes an encoded input is closest to, we gain an extra layer of interpretability for the classification process. When the number of superprototypes is equal to the number of classes, a fixed activation pattern based on closest superprototype can be used to force the detection of all classes. The subprototypes will in this case take care of intraclass variation. The network automatically learns which classes need more subprototypes: in MNIST terminology, there might be more distinct ways to write a 2 than there are ways to write a 9. This solves an issue that the original architecture \citep{li2018deep} had, namely that when the number of prototypes exceeded the number of classes, there were cases were not all classes were represented by a prototype.

We kept most of the original architecture from \citep{li2018deep} intact, only adding to it to work with hierarchical prototypes. The new architecture still consists of an autoencoder and a prototype layer. The inputs are encoded and their distances to the subprototypes and superprototypes are calculated. These two sets of distances are then used for two classifications, one for sub- and superprototypes respectively. Subprototypes are trained in exactly the same way as prototypes in the original architecture -- that is, by forcing them to be similar to encoded input through regularization. Superprototypes are forced to look similar to subprototypes instead.
