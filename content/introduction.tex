The importance of transparency in Machine Learning is rapidly increasing. As Machine Learning algorithms grow in prominence in society, being able to explain the output of these algorithms becomes more and more of a requirement. This is true both from a legal angle (such as with the GDPR \citep{gdprart}) as from a human angle -- interpretability engenders trust in users, even when it makes mistakes \citep{automtrust}, which is especially important when people rely on the algorithm to keep them healthy and safe, as is the case with medical AI or self-driving cars. Furthermore, an explainable algorithm allows us to better find biases and spurious relationships present in the training data, e.g. because of data leakage \citep{dataleakage}. However, due to the highly complex nature of artificial neural networks that most modern Machine Learning algorithms rely on, giving satisfactory explanations of model output is similarly complex.

One method of explaining why a model classifies inputs the way it does is by visualizing what a model considers to be an archetypal example of a class. Activation Maximization (AM) \citep{activationmaximization} is such a method: it provides an input that would produce the highest activation for a certain class. However, from a human point of view, such inputs tend to be dissimilar to actual inputs from that class. This goes against the intuition that model explanations should be interpretable as well as accurate. To resolve this dissimilarity, Li et al. \citep{li2018deep} propose an representation learning architecture for a \textit{prototype classifier} based around classification through measuring distance to prototypes. Prototypes are defined as points in a latent space that represent a class. These are learned, through regularization, such that they are close to encoded inputs corresponding to that class. Li et al. simultaneously train the classifier and an autoencoder \citep{autoencoderpaper} which transforms inputs to the latent space that the prototypes are in. When this training is done, the autoencoder can be used to visualize the prototypes by decoding them.

A number of other ways of addressing the lack of transparency also exist. One example of those is the explanation technique LIME \citep{LIME}, which provides local (i.e. for a single input) explanations of what features are important for the classification. When applied to images, for example, it highlights which pixels weighed heavily in its decision-making. This technique is binary -- a feature either was important or it was not. Other examples of methods that focus on explanation through feature highlighting have been recently proposed by Lundburg \& Lee \citep{featureexamplepaper1}. One drawback of the methods of improving explanation discussed so far is that they require new models to be trained in order to make explanation possible. A method that does not have this issue is Integrated Gradients, introduced by Sundararajan et al. \citep{axioms}.

In this paper, we first of all attempt to reproduce the results attained by the architecture introduced by Li et al. \citep{li2018deep}. Secondly, we extend the architecture with hierarchical prototypes. This hierarchy is composed of two layers: superprototypes and subprototypes. Subprototypes are close to actual encoded input (similar to prototypes in the original architecture), while superprototypes act as descriptors for the subprototype clusters in the latent space. For example, when training on the MNIST dataset, subprototypes can find different ways of writing the same digit (as prototypes do in \citep{li2018deep}), while superprototypes come to represent the average way numbers are written. By visualizing the sub- and superprototypes an encoded input is closest to, we gain an extra layer of interpretability for the classification process. Furthermore, if the number of superprototypes is lower than the number of classes, they can find classes that are close together. Using the MNIST dataset as an example again, the numbers 4 and 9 are similar, but different from other numbers. In this case, a superprototype could come to cover both 4s and 9s. This can aid in understanding which classes often get mixed up and why, and allows us to take steps to avoid these misclassifications. When the number of superprototypes is equal to the number of classes, a fixed activation pattern based on closest superprototype can be used to force the detection of all classes. The subprototypes will in this case take care of intraclass variation. The network automatically learns which classes need more subprototypes: in MNIST terminology, there might be more distinct ways to write a 2 than there are ways to write a 9. 

We kept most of the original architecture from \citep{li2018deep} intact, only adding to it to work with hierarchical prototypes. The new architecture still consists of an autoencoder and a prototype layer. The inputs are encoded and their distances to the subprototypes and superprototypes are calculated. These two sets of distances are then used for two classifications, one for sub- and superprototypes respectively. Subprototypes are trained in exactly the same way as prototypes in the original architecture -- that is, by forcing them to be similar to encoded input through regularization. Superprototypes are forced to look similar to subprototypes instead.
