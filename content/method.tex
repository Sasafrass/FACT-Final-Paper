\begin{itemize}
    \item Explain in detail how the algorithm works (and how we ported it from tensorflow to pytorch).
    \begin{itemize}
        \item Show the architecture image from the original paper.
    \end{itemize}
    \item Explain how we extended it for our experiment.
\end{itemize}

The original architecture from \citep{li2018deep} is comprised of two parts. These are an autoencoder and a prototype network. The autoencoder's encoder, $f$, takes a $p$-dimensional input $\textbf{x}$ and transforms it to a $q$-dimensional latent-space. The decoder, $g$, takes a $q$-dimensional input and transforms it back into $p$-dimensional space. The prototype network, $h$, takes a $q$-dimensional input and outputs $K$ probabilities, one for each of the $K$ classes. This network itself consists of a prototype layer $p : \mathbb{R}^q\rightarrow\mathbb{R}^m$, followed by a fully-connected linear layer $w : \mathbb{R}^m\rightarrow\mathbb{R}^K$ with learnable weights (unless $m = K$, see further down), and finally a softmax layer $s : \mathbb{R}^K\rightarrow\mathbb{R}^K$. The prototype layer contains $m$ learnable $q$-dimensional prototype vectors. When given an input, this layer calculates the squared $L^2$ distance between this input and each of the prototype vectors. These distances are then pushed through $w$ and then through $s$ to get the classification. In case $m = K$, $w$ is a negative identity matrix $-I_{K \times K}$ instead.

The loss is comprised of a sum of four terms: Cross-entropy for the classification, the reconstruction error of the autoencoder, and two interpretability terms $R_1$ and $R_2$, defined as the mean of the minimum of squared $L^2$ distances between every prototype and all encoded inputs and between every encoded input and all prototypes respectively. Hyperparameters can be used to set the ratio between these terms.