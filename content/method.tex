We kept most of the original architecture from \citep{li2018deep} intact, only adding to it to work with hierarchical prototypes. The new architecture still consists of an autoencoder and a prototype layer. The inputs are encoded and their distances to the subprototypes and superprototypes are calculated. These two sets of distances are then used for two classifications, one for sub- and superprototypes respectively. Subprototypes are trained in exactly the same way as prototypes in the original architecture -- that is, by forcing them to be similar to encoded input through regularization. Superprototypes are forced to look similar to subprototypes instead.

\subsection{Original architecture}
\begin{figure}[h]
    \centering
    \input{content/architecture.tex}
    \caption{Architecture of the standard prototype model \\ {\normalfont Gray nodes represent images encoded in latent space. The {\color{magenta} purple} lines indicate the similarity between the input images and prototypes.}}
    \label{fig:nonhierarchical}
\end{figure}

The original architecture from \citep{li2018deep} is comprised of two parts: an autoencoder \citep{autoencoderpaper} and a prototype network. These are shown schematically in Figure~\ref{fig:nonhierarchical}. The autoencoder's encoder, $f$, takes a $p$-dimensional input $\textbf{x}$ and transforms it to a $q$-dimensional latent-space. The decoder, $g$, takes a $q$-dimensional input and transforms it back into $p$-dimensional space. 

The prototype network, $h$, takes a $q$-dimensional input and outputs $K$ probabilities, one for each of the $K$ classes. This network itself consists of a prototype layer $p : \mathbb{R}^q\rightarrow \mathbb{R}^m$. It is  followed by a fully-connected linear layer $w : \mathbb{R}^m\rightarrow\mathbb{R}^K$ with learnable weights (unless $m = K$, see further down). Finally, there is a softmax layer $s : \mathbb{R}^K\rightarrow\mathbb{R}^K$. The prototype layer contains $m$ learnable $q$-dimensional prototype vectors. When given an input, this layer calculates the squared $L^2$ distance between a single input and each of the prototype vectors. These $m$ distances are then pushed through $w$ and $s$ to get the classification. In case $m = K$, $w$ is a negative identity matrix $-I_{K \times K}$ instead.

The loss is comprised of a sum of four terms: Cross-entropy for the classification, the reconstruction error of the autoencoder, and two interpretability terms $R_1$ and $R_2$. $R_1$ is defined as the mean of the minimum of distances between some prototype and the current input images. It forces prototypes to be similar to at least one data point in the batch. $R_2$ is defined as the mean of the minimum of distances between some datapoint in latent space and the prototypes. This term forces encoded images to be close to at least one prototype.

The final loss function is as follows:
\begin{equation}  L((f,g,h),D) = \lambda_{\text{class}}E(h \circ f, D) + \lambda_R R(g \circ f,D) + \lambda_1 R_1 + \lambda_2 R_2 \label{eq:ogloss}\end{equation}
Where $\lambda_{\text{class}}, \lambda_{R}, \lambda_{R_1}$ and $\lambda_{R_2}$ are hyperparameters that can be used to adjust the importance of the different terms. 

\subsection{Hierarchical prototype network}

In the hierarchical version of the prototype network, the single prototype layer is replaced by a subprototype layer $p : \mathbb{R}^q\rightarrow \mathbb{R}^m$ of length $m > K$ and a superprototype layer $P:\mathbb{R}^{q}\rightarrow\mathbb{R}^{k}$, where $k$ is the number of superprototypes. These two prototype layers both have their own classification output, denoted $h_{\text{sub}}$ and $h_{\text{sup}}$. When $k$ is equal to the number of classes $K$, no intermediate linear layer is learned. Instead, its output is fixed to the negative identity matrix as before. The new layered prototype network is visualized in Figure~\ref{fig:hierarchical}. 

We adjust the loss function to deal with the new layer. Error terms $R_1$ and $R_2$ force the subprototypes to be similar to at least one input data point and vice versa, as before. Two new terms, $R_3$ and $R_4$ are introduced to enforce similarity for a superprototype to at least one subprototype and vice versa. Thus, the superprototypes are connected to the subprototypes the same way the subprototypes are connected to the inputs. This highlights the hierarchicality of our network. The final loss term is:

\begin{align}
    L((f,g,h),D) &= \lambda_{sub} E_{sub}(h_{\text{sup}} \circ f, D)\label{eq:hierloss}\\ &\phantom{{}=1}+ \lambda_{sup} E_{sup}(h_{\text{sup}} \circ f, D)
    + \lambda_R R(g \circ f,D)\notag\\ &\phantom{{}=1}+ {\color{red}\lambda_1 R_1 + \lambda_2 R_2}  + {\color{blue}\lambda_3 R_3 + \lambda_4 R_4}\notag
\end{align}

Where $R_3$ and $R_4$ describe the connection between subprototypes and superprototypes, as $R_1$ and $R_2$ do in the original paper:
\begin{align*}
    R_3(\bm{P}_1, \dots, \bm{P}_K, \bm{p}_1, \dots, \bm{p}_m) &= \frac{1}{K}\sum_{k=1}^K \min_{j\in [1,m]}||\bm{P}_k- \bm{p}_j||^2_2 \\
     R_4(\bm{P}_1, \dots, \bm{P}_K, \bm{p}_1, \dots, \bm{p}_m) &= \frac{1}{m}\sum_{j=1}^m \min_{k\in [1,K]}||\bm{P}_k- \bm{p}_j||^2_2
 \end{align*}

Again, all $\lambda$'s are hyperparameters that can be used for adjusting the importance of the various error terms.

Furthermore, by using the superprototypes to classify inputs and taking the cross-entropy error of this task into account in the loss function, we ensure that the superprototypes remain useful for the main task -- that is, classification. 

\begin{figure}[hb]
    \centering
    \input{content/hierarchitecture.tex}
    \caption{Architecture for a subprototype network\\ {\normalfont Gray nodes represent images encoded in latent space. {\color{red} Red} lines indicate the similarity between the input images and the subprototypes. In contrast, {\color{blue} blue} lines indicate the similarity between the superprototypes and subprototypes. These colors also hold in Equation~\ref{eq:hierloss}. Notice that there are two classifiers, as opposed to the single classifier in Figure~\ref{fig:nonhierarchical}.}}
    \label{fig:hierarchical}
\end{figure}
