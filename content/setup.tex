To test our architecture, we apply it on the MNIST dataset to see how it performs at classifying handwritten digits in comparison to \cite{li2018deep}, using a near-identical experimental setup. Since the original supplementary material did not include a random seed, it was not possible to use the exact same setup. Thereafter, we apply our hierarchical prototype network to MNIST to see whether it will produce specialized subprototypes and generalized superprototypes.

Because our first goal is to reproduce the results attained by \citet{li2018deep}, we used the same autoencoder and hyperparameters that are used in this paper. The encoder consists of four convolutional layers of shapes $14\times 14\times 32$, $7\times 7\times 32$, $4\times 4 \times 32$ and $2\times 2\times 10$ respectively. The latent space is thus of size $2\times 2 \times 10=40$. The decoder consists of four identically sized convolutional layers in the opposite direction. The activation function for all layers of the autoencoder is the sigmoid function. 

Next, the authors set a number of hyperparameters: the number of prototype vectors, the size of the latent space and four hyperparameters $\lambda_{\text{class}}$, $\lambda_{\text{R}}$, $\lambda_1$ and $\lambda_2$ for adjusting the ratio between the reconstruction error, the $R_1$ term and the $R_2$ term respectively. To allow for reproducibility, we used values $\lambda_{\text{class}}=20$ and $\lambda_{\text{R}}=\lambda_1=\lambda_2=1$ as in \cite{li2018deep}.

Adjusting the $\lambda$ hyperparameters of our loss function will allow us to dictate which individual loss terms will have the most influence on our final loss value. The most important loss term will be the classification, because the primary aim of our architecture is achieving a high accuracy. We set $\lambda_{\text{sub}}=\lambda_{\text{sup}}=20$, similar to the nonhierarchical approach. By selecting values for $\lambda_{\text{sub}}$ and $\lambda_{\text{sup}}$ that are proportionally higher than our other regularization terms, we enforce that our crossentropy loss is of the same magnitude as the other loss terms. We set $\lambda_{1}=\lambda_{2}=\lambda_{3}=\lambda_{4}=1$. Assigning the same value to each of these terms ensures that prototypes will resemble our input data. Finally, we set our $\lambda_{R}$ to 1 to ensure good reconstruction at a minimum loss. 

For both architectures, we used the Adam optimizer \cite{adam} and a batch size of 250 for training. We selected a low learning rate of 0.0001 and trained over 1500 epochs to allow our prototypes to form gradually. The choice of batch size and number of epochs is done in accordance with \cite{li2018deep}. By opting for a lower learning rate instead of a higher one, the set of prototypes is more likely to find representations for all classes in the dataset, instead of being heavily influenced by initialization and the subset of data that our model encounters during the early stages of training. To maintain reasonable accuracy while still achieving interpretability, we apply the data augmentation technique \textit{elastic deformation} \citep{elasticdeformation} to every batch, following the same training setup as described in \citep{li2018deep} where a Gaussian filter of standard deviation 4 and scaling factor of 20 is used for the displacement field. 