To test our architecture, we apply it on the MNIST dataset to see how it performs at classifying handwritten digits in comparison to \cite{li2018deep}, using a similar experimental setup. We also want to see how interpretable the prototypes will be and if the model will produce specialized sub-prototypes and generalized super-prototypes.

Because our first goal is to reproduce the results attained by  \cite{li2018deep} we used the hyperparameters that are used in the paper. The authors set a number of hyperparameters: the number of prototype vectors, the size of the latent space and four hyperparameters $\lambda_{\text{class}}$, $\lambda_{\text{R}}$, $\lambda_1$ and $\lambda_2$ for adjusting the ratio between the reconstruction error, the R1 term and the R2 term respectively. To allow for reproducibility, we used values $\lambda_{\text{class}}=20$ and $\lambda_{\text{R}}=\lambda_1=\lambda_2=1$ as in  \cite{li2018deep}.

Adjusting the hyperparameters corresponding with the regularization of our loss function will allow us to dictate which individual loss terms will have the most influence on our final loss value. The most important loss term will be the classification given that our architecture has to maintain high accuracy. We set $\lambda_{\text{sub}}=\lambda_{\text{sup}}=20$, similar to the nonhierarchical approach. By selecting values for $\lambda_{\text{sub}}$ and $\lambda_{\text{sup}}$ that are proportionally higher than our other regularization terms, we enforce that our classification will be the most important factor during our training. We set $\lambda_{1}=\lambda_{2}=\lambda_{3}=\lambda_{4}=1$. We need to enforce that our prototypes will resemble prototypical representation of our dataset, this is achieved by assigning the same values for $\lambda_{1}$ and $\lambda_{2}$ and $\lambda_{3}$ and $\lambda_{4}$. We also set our $\lambda_{R}$ to 1 to ensure good reconstruction at a minimum loss. 

The Adam optimizer \cite{adam} was used for training with a batch size of 250. We select a low learning rate of 0.0001 over 1500 epochs to allow our prototypes to form gradually over time. The selection of batch size and number of epochs is done in accordance with \cite{li2018deep}. By selecting a low learning rate instead of a high one, our prototypes are more likely to give a prototypical representation of our dataset as a whole, instead of being heavily influenced by a subset of data that our model encounters during early stages of training. 


To maintain reasonable accuracy and at the same achieving interpretability, we apply the data augmentation technique \textit{elastic deformation}\citep{elasticdeformation} to every batch, following the same training setup as described in \citep{li2018deep} where a Gaussian filter of standard deviation 4 and scaling factor of 20 is used for the displacement field. 