\documentclass{beamer}[169]
\usepackage{wasysym}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{epsf}
\usepackage{media9}
\usetikzlibrary{decorations.pathreplacing} % Acolades
\usetikzlibrary{arrows}
\usepackage{bm}
\usefonttheme{professionalfonts}
\usetheme{PaloAlto}
\usecolortheme{spruce}
\addtobeamertemplate{footline}
{%
   \usebeamercolor[fg]{author in sidebar}
   \vskip-1cm\hskip10pt
   %\insertpagenumber\,/\,\insertpresentationendpage\kern1em\vskip2pt%
   \insertframenumber\,/\,\inserttotalframenumber\kern1em\vskip2pt%
}

\title{Towards Hierarchical Explanation}
\author{Christiaan \and Hinrik \and Albert \and Anna}
\date{FACT-AI 2020}


\begin{document}

\frame{\titlepage}

\begin{frame}{Table of Contents}
\tableofcontents
\end{frame}

\section{Reproducing the prototype network}
\begin{frame}{Original paper}
    \begin{thebibliography}{99}
        \bibitem{deep}
        Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia.
        \newblock Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions.
        \newblock {\em Thirty-Second AAAI Conference on Artificial Intelligence}, 2018
    \end{thebibliography}
\end{frame}
% Ask "who has read the paper" 

\begin{frame}{Idea}
\begin{itemize}
    \item Broadly speaking, (convolutional) neural nets are not interpretable
    \item Instead of explaining predictions after training, integrate explanations in training goal
    \item Learn a fixed amount of \alert{prototypes} which \emph{represent the entire dataset}
\end{itemize}
% This is the theme of the course, of course!
% 
% "Represent the entire dataset" -> For mnist that means,
% at least all classes of digits. (We will see an example in a bit)
\end{frame}

\begin{frame}{The prototype network}
\begin{figure}
    \input{content/slides/architecturesmall.tex}
    \end{figure}
\end{frame}
% Just point out how the two are connected


\begin{frame}{Building the loss function}
\begin{block}{Loss function}
 \only<1>{Loss = Reconstruction error}
 \only<2>{Loss = Crossentropy loss + Reconstruction error}
 \only<3-5>{Loss = Crossentropy loss + Reconstruction error + Regularization terms}
 \only<1>{\[L((f,g)),D) = R(g \circ f, D)\]}
 \only<2>{\[ L((f,g,h),D) =E(h \circ f, D) +  R(g \circ f,D)\]}
 \only<3-4>{\[ L((f,g,h),D) =E(h \circ f, D) +  R(g \circ f,D) + R_1 + R_2\]}
 \only<5>{\[ L((f,g,h),D) = \lambda_{\text{class}}E(h \circ f, D) + \lambda_R R(g \circ f,D) + \lambda_1 R_1 + \lambda_2 R_2\]}
\end{block}
 \only<4->{ \begin{block}{Regularization terms for prototypes $\bm{p_1},\dots,\bm{p_m}$}
 \begin{align*}
     R_1(\bm{p_1}, \bm{p_2}, \dots, \bm{p_m}, D) = \frac{1}{m}\sum_{j=1}^m \min_{i\in [1,n]}||\bm{p}_j- f(\bm{x}_i)||^2_2 \\
     R_2(\bm{p_1}, \bm{p_2}, \dots, \bm{p_m}, D) = \frac{1}{{\color{blue}n}}\sum_{{\color{blue}i}=1}^{{\color{blue}n}} \min_{{\color{blue}j}\in [1,{\color{blue}m}]}||\bm{p}_j- f(\bm{x}_i)||^2_2
 \end{align*}
\end{block}}
\end{frame}
% Iteratively explaining the loss terms

\begin{frame}{Reproducing results}
    \begin{itemize}
        \item MNIST digits
        \item 15 prototypes \pause 
        \item Autoencoder with four convolutional layers \pause 
        \item Learning rate 0.0001, Epochs 1500 \pause
        \item Test accuracy: 98.879\% (Paper reports 99.22\%)
    \end{itemize}
\end{frame}

\begin{frame}{Learned prototypes}
\begin{itemize}
\item Original results:\\
\includegraphics[scale=0.4]{img/originals.png}\pause
\item Reproduced results:\\
\includegraphics[scale=0.9]{img/reproduced42.png}
\end{itemize}
\end{frame}

\begin{frame}{However...}
\begin{itemize} 
\item Reproduced results with another seed:\\

\includegraphics[scale=0.9]{img/reproduced9.png}

\item (Accuracy still 98.71\%)
\end{itemize}
\end{frame}


\section{The hierarchical prototype network}
\begin{frame}{The hierarchical idea}
\begin{itemize}
    \item If $m>K$, multiple prototypes of 1 class 
    \item Sometimes prototype network does not learn a prototype for each class
    \item If $m=K$, 1 prototype for each class
    \item Cannot capture intraclass differences
\end{itemize}
\pause 
\begin{block}{Possible solution: \emph{superprototypes}}
 $ \text{Input example}\prec \text{Subprototype}\prec \text{Superprototype}$, \\
    where $\prec$ means ``more specific than"
\end{block}

    %\item \emph{More layers!} Allows network to learn each class in superprototype layer with $K$ superprototypes \pause and more specific prototypes (eg. different types of 2's) in the subprototype layer.
\end{frame}

\begin{frame}{Our architecture}
    \input{content/slides/hierarchitecturesmall.tex}
\end{frame}

\begin{frame}{Two new loss terms}
\begin{block}{New loss term}
$L((f,g,h),D) = \lambda_{\text{class}}E(h \circ f, D) + \lambda_R R(g \circ f,D) + \lambda_1 R_1 + \lambda_2 R_2 + \lambda_3 R_3 + \lambda_4 R_4$
\end{block}
 \begin{align*}
R_1(\bm{p_1}, \dots, \bm{p_m}, D) &= \frac{1}{m}\sum_{j=1}^m \min_{i\in [1,n]}||\bm{p}_j- f(\bm{x}_i)||^2_2 \\
      R_2(\bm{p_1},  \dots, \bm{p_m}, D) &= \frac{1}{{\color{red}n}}\sum_{{\color{red}i}=1}^{{\color{red}n}} \min_{{\color{red}j}\in [1,{\color{red}m}]}||\bm{p}_j- f(\bm{x}_i)||^2_2\\
     R_3(\bm{P_1}, \dots, \bm{P_K}, \bm{p_1}, \dots, \bm{p_m}) &= \frac{1}{K}\sum_{k=1}^K \min_{j\in [1,m]}||\bm{P}_k- \bm{p}_j||^2_2 \\
    R_4(\bm{P_1}, \dots, \bm{P_K}, \bm{p_1}, \dots, \bm{p_m}) &= \frac{1}{{\color{blue}m}}\sum_{{\color{blue}j}=1}^{{\color{blue}m}} \min_{{\color{blue}k}\in [1,{\color{blue}K}]}||\bm{P}_k- \bm{p}_j||^2_2
 \end{align*}
\end{frame}


\section{Hierarchical results}
\begin{frame}{Results}
\begin{itemize}
    \item Accuracy for superprototype classifier: 98.86\% %5999999999997 
    \item Accuracy for subprototype classifier:  99.02\%
\end{itemize}
\end{frame}
\begin{frame}{Superprototypes and subprototypes}
\begin{itemize}
 \item $K$ Superprototypes (fixed layer) \\   \includegraphics[scale=0.9]{img/hier42prot1499.png}

 \item $m$ Subprototypes (learnable FC layer) \\   \includegraphics[scale=0.9]{img/hier42subprot1499.png}
\end{itemize}
\end{frame}

\section{Discussion \& Broader implications}
\begin{frame}{Transparency \& Fairness}
    \begin{itemize}
        \item Some hierarchical interpretability
        \item Model interclass and intraclass variation
        \item Possibly discover biases in dataset by looking at (sub)prototypes
    \end{itemize}
\end{frame}


\begin{frame}[plain]

\advance\textwidth2cm
\hsize\textwidth
\columnwidth\textwidth
\huge
\alert{Thank you for your attention}\pause \\
Any questions?
\end{frame}



\end{document}